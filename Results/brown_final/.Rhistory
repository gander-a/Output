weight = weight1 + weight2
#Word permutations
comb = combinations(wordfrom, 2)
for (j in 1:nrow(comb)) {
word1 = comb[j,1]
word2 = comb[j,2]
word1 = gsub("\\)", "", word1)
word1 = gsub("\\(", "", word1)
word2 = gsub("\\)", "", word2)
word2 = gsub("\\(", "", word2)
newrow = as.data.frame(matrix(c(word1,word2,weight), 1, 3))
colnames(newrow) = colnames(network)
newrows = rbind(newrows, newrow)
print(newrow)
}
#New edges
word = split_and_clean(currentrow$from_word, TRUE)
if (word != wordto) {
for (ind1 in 1:length(word)) {
for (ind2 in 1:length(wordto)) {
word1 = word[ind1]
word2 = wordto[ind2]
newrow = as.data.frame(matrix(c(word1,word2,currentrow$LanguageWeight), 1, 3))
colnames(newrow) = colnames(network)
newrows = rbind(newrows, newrow)
print(newrow)
}
}
}
}
#Add edges
if (length(wordto)>1) {
word = split_and_clean(currentrow$from_word, TRUE)
if (!is.na(word)&!is.na(wordto)) {
for (j in 1:length(word)) {
word1 = word[j]
for (k in 1:length(wordto)) {
word2 = wordto[k]
newrow = as.data.frame(matrix(c(word1,word2,currentrow$LanguageWeight), 1, 3))
colnames(newrow) = colnames(network)
newrows = rbind(newrows, newrow)
print(newrow)
}
}
}
}
}
allnew = rbind(allnew, newrows)
}
#Post-process the dataframe, remove NAs, remove duplicates
allnew = allnew[!duplicated(allnew), ]
allnewbefore = allnew
allnew = allnew[complete.cases(allnew),]
allnew$weight = as.numeric(as.character(allnew$weight))
colnames(allnew) = c("from_word", "to_word", "LanguageWeight")
allnew <- aggregate(list(allnew$LanguageWeight), by = list(allnew$from_word, allnew$to_word), sum)
colnames(allnew) = c("from_word", "to_word", "LanguageWeight")
#Additional step to remove the stopwords from above once more (not sure if this is necessary)
allnew = allnew[!(allnew$from_word %in% stopwords),]
allnew = allnew[!(allnew$to_word %in% stopwords),]
allnew$from_word = gsub("\\)", "", allnew$from_word)
allnew$to_word = gsub("\\)", "", allnew$to_word)
#Store network
fname = sprintf('%sFiles/filt_%s_extended.Rda',mainpath, net)
network = allnew
save(network, file = fname)
library(igraph)
library(xtable)
library(dplyr)
library(dils)
library(xlsx)
#Setup
rm(list=ls())
mainpath = "C:/Users/Armin/Desktop/Data Science/CSH Project/Colex_vader/Output/"
setwd(paste0(mainpath, "Scripts"))
#Load networks
for (net in c("clics3_extended", "omegawiki_extended", "freedict_extended")) {
fname = sprintf('%s/Files/filt_%s.Rda', mainpath, net)
load(fname)
weight = "LanguageWeight"
network = network[,c("from_word", "to_word", weight)]
colnames(network) = c("from", "to", "weight")
network$from = as.character(network$from)
network$to = as.character(network$to)
network$weight = as.numeric(network$weight)
assign(net, network)
rm(network)
}
#Create combined network with clics3 network as a base
network = clics3_extended
allwords = c(unique(clics3_extended$from), unique(clics3_extended$to))
#Iterate through omegawiki network: If an edges both words are in clics3, add this edge
for (i in 1:nrow(omegawiki_extended)) {
currentrow = omegawiki_extended[i,]
if (currentrow$from %in% allwords & currentrow$to %in% allwords) {
network = rbind(network, currentrow)
}
}
#Iterate through freedict network: If an edges both words are in clics3, add this edge
for (i in 1:nrow(freedict_extended)) {
currentrow = freedict_extended[i,]
if (currentrow$from %in% allwords & currentrow$to %in% allwords) {
network = rbind(network, currentrow)
}
}
#Store net network
fname = sprintf('%s/Files/combined_clics3based.Rda', mainpath)
save(network, file = fname)
library(igraph)
library(xtable)
library(dplyr)
library(dils)
library(xlsx)
#Setup path
rm(list=ls())
mainpath = "C:/Users/Armin/Desktop/Data Science/CSH Project/Colex_vader/Output/"
setwd(paste0(mainpath,"Scripts"))
#Load network
net = "combined_clics3based"
fname = sprintf('%sFiles/%s.Rda', mainpath,net)
load(fname)
network_orig = network
#Select and format columns
weight = "LanguageWeight"
network = network[,c("from_word", "to_word", weight)]
network$weight = as.numeric(network$weight)
colnames(network) = c("from", "to", "weight")
#Create igraph graph
g=graph.data.frame(network)
#Get adjacency matrix
adjmat = get.adjacency(g,sparse=FALSE, attr='weight')
nodelist = colnames(adjmat)
colnames(adjmat) = nodelist
rownames(adjmat) = nodelist
#Diagonal entries are self loops -> sum of all ingoing and outgiong edges
for (i in 1:nrow(adjmat)) {
adjmat[i,i] = sum(adjmat[i,]) + sum(adjmat[,i])
}
#Loop over different beta values
betalist = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8 ,0.9)
#Load script where inverting happens
source('Networks/similarities_from_adjacency.R')
for (beta in betalist) {
#Run computation is other script
result1 = similarities_from_adjacency(adjmat, beta)
result2 = similarities_from_adjacency(t(adjmat), beta)
result = (result1 + result2)*0.5
colnames(result) = nodelist
rownames(result) = nodelist
#Store resulting adjacency matrices
storename = sprintf("%sSimilarities/simil_list_beta_%s.csv",mainpath, beta)
write.csv(result, storename)
}
library(igraph)
library(xtable)
library(dplyr)
library(dils)
library(xlsx)
#Setup path
rm(list=ls())
mainpath = "C:/Users/Armin/Desktop/Data Science/CSH Project/Colex_vader/Output/"
setwd(paste0(mainpath,"Scripts"))
#Load network
net = "combined_clics3based"
fname = sprintf('%sFiles/%s.Rda', mainpath,net)
load(fname)
network_orig = network
#Select and format columns
weight = "LanguageWeight"
network = network[,c("from_word", "to_word", weight)]
network$weight = as.numeric(network$weight)
colnames(network) = c("from", "to", "weight")
#Create igraph graph
g=graph.data.frame(network)
#Get adjacency matrix
adjmat = get.adjacency(g,sparse=FALSE, attr='weight')
nodelist = colnames(adjmat)
colnames(adjmat) = nodelist
rownames(adjmat) = nodelist
#Diagonal entries are self loops -> sum of all ingoing and outgiong edges
for (i in 1:nrow(adjmat)) {
adjmat[i,i] = sum(adjmat[i,]) + sum(adjmat[,i])
}
#Loop over different beta values
betalist = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8 ,0.9)
#Load script where inverting happens
source('Networks/similarities_from_adjacency.R')
for (beta in betalist) {
#Run computation is other script
result1 = similarities_from_adjacency(adjmat, beta)
result2 = similarities_from_adjacency(t(adjmat), beta)
result = (result1 + result2)*0.5
colnames(result) = nodelist
rownames(result) = nodelist
#Store resulting adjacency matrices
storename = sprintf("%sSimilarities/simil_list_beta_%s.csv",mainpath, beta)
write.csv(result, storename)
}
rm(list=ls())
library(xtable)
library(readr)
library(textstem)
library(ggplot2)
library(ggrepel)
library(xlsx)
library(Hmisc)
library(lsa)
library(DescTools)
library(MASS)
library(ggpubr)
library(bestNormalize)
library(boot)
#Setup directories
mainpath = "C:/Users/Armin/Desktop/Data Science/CSH Project/Colex_vader/Output/"
setwd(mainpath)
#Chose which networks and datasets to use
nets = c("combined_clics3based")
datasets = c("MEN", "SimVerb", "SimLex")
#Choose b = beta parameter, l = lower_threshold parameter
b = c(0.8)
l = c(0.5)
#Setup results dataframe
results = as.data.frame(matrix(NA,length(datasets)*length(b)*length(l),16))
colnames(results) = c("Network","Threshold","Number_Words","Dataset", "Unique_words","Number_WPs", "WPs_included","Coverage","Pearson", "Spearman", "Kendall", "Pearson_cos", "Spearman_cos", "Kendall_cos", "Pearson_Pvalue", "Spearman_Pvalue")
c=1
#Run loop over all networks
for (i in 1:length(nets)) {
source("Scripts/Computations/setup_sim_matrix.R")
#Run loop over all beta parameter values
for (beta in b) {
#Run loop over all lower_threshold parameter values
for (lower_th in l) {
net = nets[i]
sim = setup_sim_matrix(net, beta, lower_th)
#Get words in the colexification network
words = lemmatize_strings(colnames(sim))
#Load dataset
for (j in 1:length(datasets)) {
ds = datasets[j]
if (ds == "MEN") {
data = read_table2("Datasets/Wordsimilarity/MEN/MEN_dataset_natural_form_full",col_names = FALSE)
data$X3 = data$X3/50
data$sim = NA
data$cossim = NA
colnames(data) = c("word1", "word2", "rating", "sim", "cossim")
data_orig = data
}
if (ds == "SimVerb") {
data = read.delim("Datasets/Wordsimilarity/SimVerb/SimVerb-3500.txt", header=FALSE)
data = data[,c(1,2,4)]
data$V4 = as.numeric(data$V4)/10
data$sim = NA
data$cossim = NA
colnames(data) = c("word1", "word2", "rating", "sim", "cossim")
data_orig = data
}
if (ds == "SimLex") {
data = read_table2("Datasets/Wordsimilarity/SimLex-999/SimLex-999.txt")
data = data[,c(1,2,4)]
data$SimLex999 = as.numeric(data$SimLex999)/10
data$sim = NA
data$cossim = NA
colnames(data) = c("word1", "word2", "rating", "sim", "cossim")
data_orig = data
}
#Load and apply function to retrieve similarity values of word pairs
source("Scripts/Computations/find_similarity.R")
data = find_similarity(data, sim)
#For results dataframe
data$word1 = as.character(data$word1)
data$word2 = as.character(data$word2)
#Fill results dataframe
results$Network[c] = strsplit(net, "_")[[1]][1]
results$Threshold[c] = as.character(lower_th)
results$Number_Words[c] = length(words)
results$Dataset[c] = ds
results$Unique_words[c] = length(unique(c(unique(data$word1), unique(data$word2))))
results$Number_WPs[c] = nrow(data)
#Store word pairs and corresponding ratings in Excel
data_all = data
data_all$sim[data_all$sim==-1] = NA
fname = sprintf("Tables/%s_%s_quant%s.xlsx", ds, net, as.character(lower_th))
write.xlsx(data_all, fname)
#Remove 'invalid' rows
data = data[data$sim!=-1,]
data = data[data$sim!=-Inf,]
#Continue filling result statistics dataframe
results$WPs_included[c] = nrow(data)
results$Coverage[c] = as.numeric(results$WPs_included[c])/as.numeric(results$Number_WPs[c])
results$Pearson[c] = cor(data$rating, data$sim, method = "pearson")
print(results$Pearson[c])
#Perform hypothesis test to check if results are statistically significant
a = cor.test(data$rating, data$sim, method = "pearson")
print(a$p.value)
results$Pearson_Pvalue[c] = a$p.value
#Perform hypothesis test to check if results are statistically significant
results$Spearman[c] = cor(data$rating, data$sim, method = "spearman")
a = cor.test(data$rating, data$sim, method = "spearman")
results$Spearman_Pvalue[c] = a$p.value
#Fill results statistics dataframe
results$Kendall[c] = cor(data$rating, data$sim, method = "kendall")
results$Pearson_cos[c] = cor(data$rating, data$cossim, method = "pearson")
results$Spearman_cos[c] = cor(data$rating, data$cossim, method = "spearman")
results$Kendall_cos[c] = cor(data$rating, data$cossim, method = "kendall")
# texts = paste0(data$word1, " ", data$word2)
# top10 = data[order(data$sim, decreasing = TRUE),][1:10,]
# texts[!(data$sim %in% top10$sim)] = ""
# data$texts = texts
#Select color of scatterplots
if (ds=="MEN") {
col = "#1b9e77"
}
if (ds == "SimVerb") {
col = "#d95f02"
}
if (ds == "SimLex") {
col = "#7570b3"
}
#Plot scatterplot
g = ggplot(data, aes(x=rating, y = sim)) +
geom_point(alpha = 0.6, position="identity", color = col, size = 8) +
geom_smooth(color = col, fill = col, size = 3) +
# geom_text_repel(hjust=0, vjust=0,size=10)+
labs(y="Predicted similarity value",
x="Ground truth rating",
title=sprintf("%s", ds),
subtitle = sprintf("Pearson: %s, Spearman: %s", round(results$Pearson[c], digits = 4), round(cor(data$rating, data$sim, method = "spearman"), digits = 4))) +
theme(panel.grid.minor = element_blank(),panel.background = element_blank(),
axis.line = element_line(colour = "black"))+
theme(text = element_text(size = 35)) +
theme(legend.position = "none")
plot(g)
pngname = sprintf("Plots/%s_%s_quant%s_similarity_%s.png", ds, net, as.character(lower_th), beta)
ggsave(pngname, width = 30, height = 20, units = "cm")
#Plot distributions of predicted and ground truth values
g = ggplot() +
geom_histogram(aes(x = data$sim, fill="b", colour="b"), alpha = 0.5) +
geom_histogram(aes(x = data$rating, fill="g", colour="g") ,alpha = 0.5) +
geom_smooth() +
labs(y="Frequency",
x="Similarity",
title=sprintf("%s", ds)) +
theme(panel.grid.minor = element_blank(),panel.background = element_blank(),
axis.line = element_line(colour = "black"))+
theme(text = element_text(size = 35)) +
scale_colour_manual(name="Similarity", values=c("g" = "gray", "b"=col, "r" = "red"), labels=c("b"="Similarity matrix", "g"="Ground truth rating", "r" = "Cosine sim")) +
scale_fill_manual(name="Similarity", values=c("g" = "gray", "b"=col, "r" = "red"), labels=c("b"="Similarity matrix", "g"="Ground truth rating", "r" = "Cosine sim"))
plot(g)
pngname = sprintf("Plots/%s_%s_th%s_histogram_similarity_%s.png", ds, net, as.character(lower_th), beta)
ggsave(pngname, width = 30, height = 20, units = "cm")
c = c+1
}
}
}
}
#Store final table of results
suppressMessages(write.xlsx(results,sprintf("Tables/Word_similarity_summary.xlsx") ,row.names = TRUE))
#Setup directories
mainpath = "C:/Users/Armin/Desktop/Data Science/CSH Project/Colex_vader/Output/"
setwd(mainpath)
#MULTIPLE RESULT FILES
folders = c("brown_final", "baseline_final")
folder = folders[1]
setwd(sprintf("%s/Results/%s",mainpath, folder))
rm(list=ls())
library(ggplot2)
library(reshape2)
library(ggalluvial)
library(dplyr)
library(plyr)
#Setup directories
mainpath = "C:/Users/Armin/Desktop/Data Science/CSH Project/Colex_vader/Output/"
setwd(mainpath)
#MULTIPLE RESULT FILES
folders = c("brown_final", "baseline_final")
for (folder in folders) {
setwd(sprintf("%s/Results/%s", mainpath, folder))
f = list.files()
for (i in 1:length(f)) {
if (i==1) {
name = substr((f[i]), 1, 100)
summary = read.csv(f[i], row.names=NULL, sep="")
summary = summary[,-c(1)]
rownames(summary) = colnames(summary)
summary[is.na(summary)]=0
check=as.matrix(summary)
check[,]=NA
check[summary!=0]=1
} else {
d = read.csv(f[i], row.names=NULL, sep="")
d = d[,-c(1)]
d[is.na(d)]=0
summary=summary+d
check[d!=0]=1
}
}
m = as.matrix(summary)
diag(m) = 0
summary = as.data.frame(m)
groundtruth = unlist(lapply(strsplit(colnames(summary), "[.]"), `[[`, 1))
knn = as.data.frame(matrix(NA, (nrow(summary)-1), (nrow(summary))))
predlabel = c()
colnames(knn) = rownames(summary)
summary_orig = summary
summary_orig$genre = groundtruth
summary_orig$index = c(1:nrow(summary_orig))
plot = 0
#Loop for bootstrap sample
maxiter = 12#240
for (iter in 1:maxiter){
print(iter)
set.seed(iter)
if (iter < maxiter) {
rpt = TRUE
} else {
rpt = FALSE
}
s = ddply(summary_orig,.(genre), function(x) x[sample(nrow(x), nrow(summary_orig)/length(unique(groundtruth)), replace = rpt),])
summary = s[,1:nrow(s)]
rownames(summary) = colnames(summary)
#Compute evaluation values
for (i in 1:nrow(summary)) {
ordered = summary[order(summary[,i], decreasing = TRUE),]
truelabel = strsplit(colnames(summary)[i], "[.]")[[1]][1]
# print(truelabel)
for (k in 1:(nrow(summary)/2)) {
labels = rownames(ordered)[1:k]
values = ordered[1:k,i]
knn[k,i] = sum(grepl(truelabel, labels))/k
if (k == 1) {
predlabel = c(predlabel, strsplit(labels[k], "[.]")[[1]][1])
}
}
}
knn = knn[,order(colnames(knn))]
knnavg = as.data.frame(rowMeans(knn, na.rm = T))
colnames(knnavg) = c("Precision")
if (iter == 1) {
res = knnavg
} else {
res = cbind(res, knnavg)
}
#Individual plots
if (plot == 1) {
#Raster plot
knnmelt = melt(knn)
knnmelt$k = rep(1:k, nrow(summary))
xlab = substr(colnames(summary), 1, nchar(colnames(summary))-3)
g = ggplot(data = knnmelt, aes(x=variable, y=k, fill=value)) +
geom_raster(stat = "identity") +
labs(y="k nearest neighbors",
x="Label",
title=sprintf("Accuracy")) +
theme(panel.grid.minor = element_blank(),panel.background = element_blank(),
axis.line = element_line(colour = "black"),axis.text.x = element_text(angle=90),
axis.text=element_text(size=15))+
theme(text = element_text(size = 25), plot.title = element_text(size=25)) +
scale_x_discrete(labels = xlab[!is.na(xlab)]) +
scale_fill_gradientn(colours=c("#0000FFFF","#FFFFFFFF","#FF0000FF"))
plot(g)
pngname = sprintf("Plots/carpet_%s.png", folder)
ggsave(pngname, width = 30, height = 20, units = "cm")
}
}
res$average = rowMeans(res)
res = res[1:(nrow(summary)/2),]
assign(paste0("res_", folder), res[!is.na(res$average),])
#kNN development plot
maxpoint = (max(res$average))
assign(paste0(substr(folder, 4, 40),"maxpoint"), maxpoint)
assign(paste0(substr(folder, 4, 40),"data"), res$average)
res$sd = apply(res,1, sd, na.rm = TRUE)
g = ggplot() +
geom_ribbon(aes(x = 1:nrow(res), ymin = res$average-2*res$sd, ymax = res$average+2*res$sd), fill = "grey70") +
geom_line(aes(x = 1:nrow(res), y=res$average), size = 3) +
geom_point(aes(x = which(maxpoint==res$average), y=maxpoint), size = 10) +
geom_text(aes(x = which(maxpoint==res$average)[1]+round(nrow(res)*0.15), y=maxpoint, label = as.character(round(maxpoint, digits = 4))), size = 15) +
labs(y="Accuracy",
x="k nearest neighbors",
title=sprintf("Average Accuracy"),
subtitle = sprintf("k:%s - sd:%s" ,which(maxpoint==res$average)[1], round(res$sd[which(maxpoint==res$average)], digits = 4))) +
theme(panel.grid.minor = element_blank(),panel.background = element_blank(),
axis.line = element_line(colour = "black"), axis.text.x = element_text(angle=90),
axis.text=element_text(size=40))+
theme(text = element_text(size = 40))+
scale_x_continuous(breaks = c(0,25,50,75,100))
plot(g)
pngname = sprintf("Plots/kNNa_%s.png", folder)
ggsave(pngname, width = 30, height = 20, units = "cm")
#Bipartite graph
d = as.data.frame(matrix(NA, length(predlabel), 0))
d$Category = groundtruth
d$Predicted = predlabel
d$Weight = 1
d = d %>% dplyr::group_by(Category, Predicted) %>% dplyr::summarise(Weight = sum(Weight))
g = ggplot(as.data.frame(d),
aes(y = Weight, axis1 = Category, axis2 = Predicted)) +
geom_alluvium(aes(fill = Category), width = 1/12, knot.pos = 0.4) +
geom_stratum(width = 1/5, fill = "white", color = "black") +
geom_label(stat = "stratum", aes(label = after_stat(stratum)), size = 6) +
scale_x_discrete(limits = c("Ground truth", "Predicted"), expand = c(.05, .05)) +
scale_fill_brewer(type = "qual", palette = "Set1") +
ggtitle("Classification of labels") +
theme(panel.grid.minor = element_blank(),panel.background = element_blank(),
axis.line = element_line(colour = "black"),
axis.text.y=element_blank(), axis.title.y = element_blank(),
axis.line.x = element_blank(), axis.line.y = element_blank())+
theme(text = element_text(size = 25))
plot(g)
pngname = sprintf("Plots/flow_%s.png", folder)
ggsave(pngname, width = 30, height = 20, units = "cm")
}
